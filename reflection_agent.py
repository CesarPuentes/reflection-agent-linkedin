from typing import List, Sequence
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_ollama import ChatOllama
from langgraph.graph import END, MessageGraph

# 1. Initialize the LLM (Ollama)
llm = ChatOllama(
    model="qwen2.5:7b",
    temperature=0.5
)

# 2. Define Generation Prompt
generation_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You are a professional LinkedIn content assistant tasked with crafting engaging, insightful, and well-structured LinkedIn posts."
            " Generate the best LinkedIn post possible for the user's request."
            " If the user provides feedback or critique, respond with a refined version of your previous attempts, improving clarity, tone, or engagement as needed.",
        ),
        MessagesPlaceholder(variable_name="messages"),
    ]
)

# 3. Create Generate Chain
generate_chain = generation_prompt | llm

# 4. Define Reflection Prompt
reflection_prompt = ChatPromptTemplate.from_messages([
    (
        "system",
        """You are a professional LinkedIn content strategist and thought leadership expert. Your task is to critically evaluate the given LinkedIn post and provide a comprehensive critique. Follow these guidelines:

        1. Assess the post’s overall quality, professionalism, and alignment with LinkedIn best practices.
        2. Evaluate the structure, tone, clarity, and readability of the post.
        3. Analyze the post’s potential for engagement (likes, comments, shares) and its effectiveness in building professional credibility.
        4. Consider the post’s relevance to the author’s industry, audience, or current trends.
        5. Examine the use of formatting (e.g., line breaks, bullet points), hashtags, mentions, and media (if any).
        6. Evaluate the effectiveness of any call-to-action or takeaway.

        Provide a detailed critique that includes:
        - A brief explanation of the post’s strengths and weaknesses.
        - Specific areas that could be improved.
        - Actionable suggestions for enhancing clarity, engagement, and professionalism.

        Your critique will be used to improve the post in the next revision step, so ensure your feedback is thoughtful, constructive, and practical.
        """
    ),
    MessagesPlaceholder(variable_name="messages")
])

# 5. Create Reflect Chain
reflect_chain = reflection_prompt | llm

# 6. Define Nodes
def generation_node(state: Sequence[BaseMessage]) -> List[BaseMessage]:
    # Pass the entire history (or relevant part) to the generation chain
    generated_post = generate_chain.invoke({"messages": state})
    return [AIMessage(content=generated_post.content)]

def reflection_node(messages: Sequence[BaseMessage]) -> List[BaseMessage]:
    # Reflection acts on the messages (which includes the latest generated post)
    res = reflect_chain.invoke({"messages": messages})
    return [HumanMessage(content=res.content)]

# 7. Define Conditional Logic
def should_continue(state: List[BaseMessage]):
    # Limit recursion/iterations
    if len(state) > 6:
        return END
    return "reflect"

# 8. Build the Graph
graph = MessageGraph()

graph.add_node("generate", generation_node)
graph.add_node("reflect", reflection_node)

graph.set_entry_point("generate")

graph.add_conditional_edges("generate", should_continue)
graph.add_edge("reflect", "generate")

# 9. Compile
workflow = graph.compile()

if __name__ == "__main__":
    print("Starting Reflection Agent Workflow...")
    
    # Initial request
    initial_input = HumanMessage(content="Write a linkedin post on getting a software developer job at IBM under 160 characters")
    
    # Execute workflow
    # We invoke with a list containing the initial message
    final_state = workflow.invoke([initial_input])
    
    print("\nWorkflow Finished.")
    print("-" * 50)
    
    # Print the conversation history to see the generation -> reflection -> refinement loop
    for i, msg in enumerate(final_state):
        role = "Unknown"
        if isinstance(msg, HumanMessage):
            # In this graph, HumanMessage can be the Initial User Input OR the Reflection Feedback
            # We can infer which is which by index, or just print content.
            # Index 0 is the User Input.
            # Indices generated by 'reflect' node are also HumanMessages (as per the pattern).
            if i == 0:
                role = "User Input"
            else:
                role = "Reflection Criterion"
        elif isinstance(msg, AIMessage):
            role = "Generated Post"
        
        print(f"\n[{i}] {role}:")
        print(msg.content)
        print("-" * 20)
